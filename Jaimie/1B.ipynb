{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"SemEval2018-Task3/datasets/train/SemEval2018-T3-train-taskB.txt\", delimiter='\\t', quoting=csv.QUOTE_NONE, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Classification Task Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances:\n",
      " 0    1923\n",
      "1    1390\n",
      "2     316\n",
      "3     205\n",
      "Name: Label, dtype: int64\n",
      "Relative label frequency:\n",
      " 0    0.501565\n",
      "1    0.362546\n",
      "2    0.082420\n",
      "3    0.053469\n",
      "Name: Label, dtype: float64\n",
      "Label:  1 example sentence:  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
      "\n",
      "\n",
      "Label:  0 example sentence:  3 episodes left I'm dying over here\n",
      "\n",
      "\n",
      "Label:  2 example sentence:  \"I can't breathe!\" was chosen as the most notable quote of the year in an annual list released by a Yale University librarian \n",
      "\n",
      "\n",
      "Label:  3 example sentence:  @yWTorres9 time to hit the books then \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Instances:\\n',dataset['Label'].value_counts())\n",
    "print('Relative label frequency:\\n',dataset['Label'].value_counts(normalize=True))\n",
    "for label in dataset['Label'].unique():\n",
    "    df_label = dataset.loc[dataset['Label']==label]\n",
    "    print('Label: ', label, 'example sentence: ', df_label['Tweet text'].iloc[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"SemEval2018-Task3/datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt\", delimiter='\\t', quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "y_true = test_set['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(test_set.Label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros(4)\n",
    "prec = np.zeros(4)\n",
    "rec = np.zeros(4)\n",
    "f1_score = np.zeros(4)\n",
    "macro_avg = np.zeros(4)\n",
    "weighted_avg = np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    y_pred = random.choices(test_set['Label'].unique(), k=len(test_set))\n",
    "    acc_category = np.diagonal(confusion_matrix(y_true, y_pred, normalize='true')) # accuracy per class\n",
    "    acc += acc_category\n",
    "    acc_pooled = accuracy_score(y_true, y_pred) # overall accuracy\n",
    "\n",
    "\n",
    "    class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    # update arrays by indexing over classification report dict\n",
    "    for i in range(4):\n",
    "        prec[i] += class_report[str(i)]['precision']\n",
    "        rec[i] += class_report[str(i)]['recall']\n",
    "        f1_score[i] += class_report[str(i)]['f1-score']\n",
    "\n",
    "\n",
    "    macro_avg_values =  list(class_report['macro avg'].values())[:3] # retrieve macro avg values for precision, recall, f-1\n",
    "    macro_avg_values.insert(0, np.mean(acc_category)) # add macro avg accuracy\n",
    "    macro_avg += np.array(macro_avg_values)\n",
    "\n",
    "    weighted_avg_values = list(class_report['weighted avg'].values())[:3] # retrieve weighted avg values for precision, recall, f-1\n",
    "    weighted_avg_values.insert(0, acc_pooled) # add weighted avg accuracy\n",
    "    weighted_avg += np.array(weighted_avg_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for all classes:  [0.25114165 0.24908537 0.25047059 0.23693548]\n",
      "Precision for all classes:  [0.60453637 0.20964374 0.10720716 0.07587426]\n",
      "Recall for all classes:  [0.25114165 0.24908537 0.25047059 0.23693548]\n",
      "F1-score for all classes:  [0.3545478  0.22740452 0.14999612 0.11486054]\n",
      "Macro average for accuracy, precision, recall, F1-score:  [0.24690827 0.24931538 0.24690827 0.21170224]\n",
      "Weighted average for accuracy, precision, recall, F1-score:  [0.24951531 0.42620419 0.24951531 0.28681948]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for all classes: ', acc/100)\n",
    "print('Precision for all classes: ', prec/100)\n",
    "print('Recall for all classes: ', rec/100)\n",
    "print('F1-score for all classes: ', f1_score/100)\n",
    "print('Macro average for accuracy, precision, recall, F1-score: ', macro_avg/100)\n",
    "print('Weighted average for accuracy, precision, recall, F1-score: ', weighted_avg/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = test_set['Label'].value_counts().argmax()\n",
    "y_pred = [majority_class]*len(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros(4)\n",
    "prec = np.zeros(4)\n",
    "rec = np.zeros(4)\n",
    "f1_score = np.zeros(4)\n",
    "macro_avg = np.zeros(4)\n",
    "weighted_avg = np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaimie\\Anaconda3\\envs\\NLPtech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jaimie\\Anaconda3\\envs\\NLPtech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jaimie\\Anaconda3\\envs\\NLPtech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc_category = np.diagonal(confusion_matrix(y_true, y_pred, normalize='true')) # accuracy per class\n",
    "acc += acc_category\n",
    "acc_pooled = accuracy_score(y_true, y_pred) # overall accuracy\n",
    "\n",
    "\n",
    "class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# update arrays by indexing over classification report dict\n",
    "for i in range(4):\n",
    "    prec[i] += class_report[str(i)]['precision']\n",
    "    rec[i] += class_report[str(i)]['recall']\n",
    "    f1_score[i] += class_report[str(i)]['f1-score']\n",
    "\n",
    "\n",
    "macro_avg_values =  list(class_report['macro avg'].values())[:3] # retrieve macro avg values for precision, recall, f-1\n",
    "macro_avg_values.insert(0, np.mean(acc_category)) # add macro avg accuracy\n",
    "macro_avg += np.array(macro_avg_values)\n",
    "\n",
    "weighted_avg_values = list(class_report['weighted avg'].values())[:3] # retrieve weighted avg values for precision, recall, f-1\n",
    "weighted_avg_values.insert(0, acc_pooled) # add weighted avg accuracy\n",
    "weighted_avg += np.array(weighted_avg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for all classes:  [1. 0. 0. 0.]\n",
      "Precision for all classes:  [0.60331633 0.         0.         0.        ]\n",
      "Recall for all classes:  [1. 0. 0. 0.]\n",
      "F1-score for all classes:  [0.75258552 0.         0.         0.        ]\n",
      "Macro average for accuracy, precision, recall, F1-score:  [0.25       0.15082908 0.25       0.18814638]\n",
      "Weighted average for accuracy, precision, recall, F1-score:  [0.60331633 0.36399059 0.60331633 0.45404713]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for all classes: ', acc)\n",
    "print('Precision for all classes: ', prec)\n",
    "print('Recall for all classes: ', rec)\n",
    "print('F1-score for all classes: ', f1_score)\n",
    "print('Macro average for accuracy, precision, recall, F1-score: ', macro_avg)\n",
    "print('Weighted average for accuracy, precision, recall, F1-score: ', weighted_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtech",
   "language": "python",
   "name": "nlptech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
